{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     18,
     43
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. Get X and y\n",
    "2. Split into training and testing data\n",
    "3. Reshape X -> (samples, channels, x-axis, y-axis)\n",
    "   Reshape y -> (sampples, x-axis * y-axis)\n",
    "4. Create ReconNet Units\n",
    "   Figure out the zero padding post each step to maintain a 33 * 33 filter size \n",
    "\"\"\"\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "np.random.seed(7)\n",
    "%matplotlib inline\n",
    "\n",
    "def get_data_tfformat(url_data_X, url_data_y):\n",
    "    print ('0. --------------------- USING TF FORMAT ---------------------  ')\n",
    "    with open(url_data_X, 'rb') as handle:\n",
    "        X = joblib.load(handle)\n",
    "        \n",
    "    with open(url_data_y, 'rb') as handle:\n",
    "        y = joblib.load(handle)\n",
    "    \n",
    "    \n",
    "    idxs = np.random.randint(0, len(X), 3)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].imshow(X[idxs[0]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[1].imshow(X[idxs[1]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[2].imshow(X[idxs[2]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].imshow(y[idxs[0]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[1].imshow(y[idxs[1]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[2].imshow(y[idxs[2]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    \n",
    "    X = X.reshape((X.shape[0], 1, 33, 33))\n",
    "    print ('1. X:', X.shape, ' y:', y.shape)\n",
    "    return X , y\n",
    "\n",
    "def get_data_theanoformat(url_data_X, url_data_y):\n",
    "    print ('0. --------------------- USING THEANO FORMAT ---------------------  ')\n",
    "    with open(url_data_X, 'rb') as handle:\n",
    "        X = joblib.load(handle)\n",
    "        X = X.reshape((X.shape[0], 33, 33, 1))\n",
    "        \n",
    "    with open(url_data_y, 'rb') as handle:\n",
    "        y = joblib.load(handle)\n",
    "    \n",
    "    \n",
    "    idxs = np.random.randint(0, len(X), 3)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].imshow(X[idxs[0]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[1].imshow(X[idxs[1]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[2].imshow(X[idxs[2]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].imshow(y[idxs[0]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[1].imshow(y[idxs[1]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    axarr[2].imshow(y[idxs[2]].reshape((33,33)), cmap = plt.cm.gray)\n",
    "    \n",
    "    print ('1. X:', X.shape, ' y:', y.shape)\n",
    "    return X , y\n",
    "\n",
    "if __name__ == \"__main__\" :\n",
    "    url_data_X = '../data/data_patches_X.gz'\n",
    "    url_data_y = '../data/data_patches_y.gz'\n",
    "    # X, y = get_data_theanoformat(url_data_X, url_data_y)\n",
    "    X, y = get_data_tfformat(url_data_X, url_data_y)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.0)\n",
    "    print ('1. X_train:', X_train.shape, ' y_train:', y_train.shape)\n",
    "    print ('1. X_test:', X_test.shape, ' y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     26,
     78,
     107
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "def check_gpu():\n",
    "    import os\n",
    "    print ('0. Envs : CUDA_HOME', os.environ['CUDA_HOME'])\n",
    "    print ('0. Envs : CUDA_ROOT',os.environ['CUDA_ROOT'])\n",
    "    print ('0. Envs : LD_LIBRARY_PATH:', os.environ['LD_LIBRARY_PATH'])\n",
    "    print ('0. Envs : PATH (containing cuda)', [each for each in os.environ['PATH'].split(':') if each.find('cuda') > -1])\n",
    "\n",
    "    from keras import backend as K\n",
    "    K.clear_session()\n",
    "    \n",
    "    print ('0. Keras backend:', K.backend())\n",
    "    if K.backend() == 'tensorflow':\n",
    "        from tensorflow.python.client import device_lib\n",
    "        devices = device_lib.list_local_devices()\n",
    "        for device in devices:\n",
    "            print ('0. TensorFlow Devices:', str(device).replace('\\n',''))\n",
    "        print ('\\n')\n",
    "        \n",
    "        return 1 if len(devices) > 1 else 0\n",
    "    \n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def model_train(X_train, y_train, model_filename_weights, epochs = 1, batch_size = 32, callbacks_list=[]):\n",
    "    ## CNN STRUCTURE ###\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "    from keras.layers import Convolution2D, MaxPooling2D\n",
    "    \n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Convolution2D(64, kernel_size=(11, 11), activation='relu', use_bias = True\n",
    "                                , input_shape=(1,33,33), data_format='channels_first', padding='same')\n",
    "                                # , input_shape=(33,33,1), data_format='channels_last', padding='same')\n",
    "                            )\n",
    "    print ('First layer:', model.output_shape)\n",
    "    # print ('First layer:', model.layers[0].output_shape)\n",
    "\n",
    "    model.add(Convolution2D(32, kernel_size=(1, 1), activation='relu', use_bias = True, padding='same'))\n",
    "\n",
    "    model.add(Convolution2D(1, kernel_size=(7, 7), activation='relu', use_bias = True, padding='same'))\n",
    "\n",
    "    model.add(Convolution2D(64, kernel_size=(11, 11), activation='relu', use_bias = True, padding='same'))\n",
    "\n",
    "    model.add(Convolution2D(32, kernel_size=(1, 1), activation='relu', use_bias = True, padding='same'))\n",
    "\n",
    "    model.add(Convolution2D(1, kernel_size=(7, 7), activation='relu', use_bias = True, padding='same'))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    \n",
    "    ## CHECK IF NEW / OLDER MODEL ##\n",
    "    if model_filename_weights == '':\n",
    "        pass\n",
    "    else:\n",
    "        model.load_weights(model_filename_weights)\n",
    "\n",
    "    ## CONTINUE WITH COMPILATION AND TRAINING ##\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    X_train_model = X_train.astype('float32')\n",
    "    X_train_model /= 255  \n",
    "    y_train_model = y_train.astype('float32')\n",
    "    y_train_model /= 255\n",
    "\n",
    "    history = model.fit(X_train_model, y_train_model, batch_size=batch_size, epochs = epochs\n",
    "                      , shuffle=True\n",
    "                      , validation_split=0.1\n",
    "                      , callbacks=callbacks_list\n",
    "                      , verbose=1)\n",
    "        \n",
    "    return model, history\n",
    "    \n",
    "\n",
    "def model_validate(model, X_test, y_test):\n",
    "    print ('\\n4. --------------------------------------> Validating model')\n",
    "    X_test_model = X_test.astype('float32')\n",
    "    X_test_model /= 255  \n",
    "    y_test_model = y_test.astype('float32')\n",
    "    y_test_model /= 255\n",
    "    metrics = model.evaluate(X_test_model, y_test_model, verbose=1)\n",
    "    print ('\\nValidation Metrics:', metrics)    \n",
    "\n",
    "def model_disk(action, filename_model_arch, filename_model_weights, model=''):\n",
    "    from keras.models import model_from_json\n",
    "    \n",
    "    print ('\\n3. --------------------------------------> Model on Disk')\n",
    "    if action == 'save':\n",
    "        with open(filename_model_arch, \"w\") as handle:\n",
    "            handle.write(model.to_json())\n",
    "        model.save_weights(filename_model_weights)\n",
    "        print(\"\\nSaved model to disk\")\n",
    "        \n",
    "    elif action == 'load':\n",
    "        json_file = open(filename_model_arch, 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        model = model_from_json(loaded_model_json)\n",
    "        model.load_weights(filename_model_weights)\n",
    "        print(\"Loaded model from disk\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def model_run(MODEL_NEW, TEST, EPOCHS, BATCH_SIZE, model_filename_weights_best):\n",
    "    ## CHECK TENSORFLOW DEVICES ##\n",
    "    if (check_gpu()):\n",
    "        from keras.callbacks import ModelCheckpoint\n",
    "        \n",
    "        ## 1.1 TRAIN MODEL : NEW MODEL OR OLD ##\n",
    "        if MODEL_NEW == 1:\n",
    "            print ('2. --------------------------------------> Fresh model')\n",
    "            model_filename_weights_best = ''\n",
    "        else:\n",
    "            print ('2. --------------------------------------> Updating model(',model_filename_weights_best,')')\n",
    "            model_filename_weights_best = model_filename_weights_best\n",
    "\n",
    "        ## 1.2 TRAIN MODEL : CREATE CHECKPOINTING INFO ##\n",
    "        model_filename_weights = \"model/index_network_euclidean_keras_weights_{epoch:02d}_{val_acc:.2f}.hdf5\"\n",
    "        # model_filename_weights = \"model/index_network_euclidean_keras_weights_{epoch:02d}.h5\"\n",
    "        checkpoint = ModelCheckpoint(model_filename_weights, monitor='val_acc', mode='auto'\n",
    "                                     # , save_best_only = True\n",
    "                                     , save_best_only = False\n",
    "                                     , save_weights_only = True, verbose=0.1)\n",
    "        callbacks_list = [checkpoint]\n",
    "\n",
    "        ## 1.3 TRAIN MODEL : TEST OR PROD MODEL ##\n",
    "        if TEST == 1:\n",
    "            model, history = model_train(X_train[:BATCH_SIZE*3], y_train[:BATCH_SIZE*3], model_filename_weights_best, epochs = EPOCHS, batch_size = BATCH_SIZE\n",
    "                                , callbacks_list = callbacks_list)\n",
    "        else:\n",
    "            model, history = model_train(X_train, y_train, model_filename_weights_best, epochs = EPOCHS, batch_size = BATCH_SIZE\n",
    "                                , callbacks_list = callbacks_list)\n",
    "\n",
    "        ## 1.4 TRAIN MODEL : SAVE MODEL AND WEIGHTS FOR ANY FUTURE REFERENCES  ##\n",
    "        filename_model_arch = 'model/index_network_euclidean_keras_model.json'\n",
    "        filename_model_weights = 'model/index_network_euclidean_keras_weights.h5'\n",
    "        model = model_disk('save', filename_model_arch, filename_model_weights, model)\n",
    "        model_validate(model, X_test, y_test)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    else:\n",
    "        print ('0. GPU not available :( ')\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    pass\n",
    "    # check_gpu()\n",
    "    \n",
    "#     MODEL_NEW = 0\n",
    "#     TEST = 0\n",
    "#     EPOCHS = 60\n",
    "#     BATCH_SIZE = 128\n",
    "    \n",
    "#     if MODEL_NEW == 1:\n",
    "#         model_filename_weights_best = ''\n",
    "#     else:\n",
    "#         model_filename_weights_best = sorted(glob('model/index_network_euclidean_keras_weights_*')\n",
    "#                                              , key=os.path.getmtime)[-1]\n",
    "#         # model_filename_weights_best = 'model/index_network_euclidean_keras_weights_09.h5'\n",
    "    \n",
    "#     model = model_run(MODEL_NEW, TEST, EPOCHS, BATCH_SIZE, model_filename_weights_best)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def get_data_random(url_data_X, url_data_y):\n",
    "    with open(url_data_X, 'rb') as handle:\n",
    "        X = joblib.load(handle)\n",
    "        \n",
    "    with open(url_data_y, 'rb') as handle:\n",
    "        y = joblib.load(handle)\n",
    "    \n",
    "    rand_idx = np.random.randint(0, len(X))\n",
    "    \n",
    "    return X[rand_idx], y[rand_idx]\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = model_disk('load', 'model/index_network_euclidean_keras_model.json'\n",
    "                               , 'model/index_network_euclidean_keras_weights.h5'\n",
    "                               , ''\n",
    "                       )\n",
    "    url_data_X = '../data/data_patches_X.gz'\n",
    "    url_data_y = '../data/data_patches_y.gz'\n",
    "    X_rand, y_rand = get_data_random(url_data_X, url_data_y)\n",
    "    print (X_rand.shape, y_rand.shape)\n",
    "    \n",
    "    X_predict = model.predict(X_rand.reshape(1,1,33,33))\n",
    "    print ('X_predict:', X_predict.shape)\n",
    "    \n",
    "    f, axarr = plt.subplots(1,3)\n",
    "    axarr[0].imshow(X_rand.reshape(33,33), cmap = plt.cm.gray)\n",
    "    axarr[1].imshow(y_rand.reshape(33,33), cmap = plt.cm.gray)\n",
    "    axarr[2].imshow(X_predict.reshape(33, 33), cmap = plt.cm.gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "K.function([K.learning_phase(), model.layers[0].input], model.layers[2].output)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
